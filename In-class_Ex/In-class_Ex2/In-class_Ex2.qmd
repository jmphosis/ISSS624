---
title: "In-class Exercise 2"
date: "25 November 2023"
date-modified: "last-modified" #allows for updating to the latest date
format: html
execute: 
  echo: true #shows the code
  eval: true #shows the outcomes of the code
  warning: false #does not show the warnings
editor: visual
background-color: lightgrey;
font-family:  Palatino Linotype;
---

***Note***: This In-class Exercise 2 combines the following three in-class exercises on isss624.netlify.app:

1.  In-class Exercise 2: Spatial Weights - sfdep methods

2.  In-class Exercise 2: Global and Local Measures of Spatial Association - sfdep methods

3.  In-class Exercise 2: Emerging Hot Spot Analysis: sfdep methods

## 1 Overview

This in-class exercise introduces an alternative R package, **sfdep**. According to Josiah Perry, the developer of the package, "*sfdep builds on the great shoulders of spdep package for spatial dependence. sfdep creates an sf and tidyverse friendly interface to the package as well as introduces new functionality that is not present in spdep. sfdep utilizes list columns extensively to make this interface possible.*"

## 2 Getting Started - Installing and Loading the R Packages

The code chunk below loads the packages used in this in-class exercise:

-   **sf** for importing, managing, and processing geospatial data;

-   **tidyverse** (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;

-   **tmap** for thematic mapping;

-   **knitr** for embedding R code in different document formats (e.g., HTML) to facilitate dynamic report generation;

-   **sfdep** for analysing spatial dependence and spatial relationships in data (building on spdep); and

-   **plotly** for making interactive plots.

```{r}
pacman::p_load(sf, tmap, sfdep, tidyverse, knitr, plotly)
```

## 3 Importing Data

For the purpose of the in-class exercise, the Hunan data sets are used:

-   [Hunan's County Boundary Layer]{.underline}. This is a geospatial data set in ESRI shapefile format.

-   [Hunan's Local Development Indicators 2012]{.underline}. This csv file contains data on selected Hunan's local development indicators in 2012.

The data sets are placed under two sub-folders:

-   geospatial (County Boundary Layer), and

-   aspatial (Local Development Indicators 2012).

These two sub-folders are within the data folder of my In-class_Ex2 folder.

### 3.1 Importing shapefile

The **st_read()** (under sf package) is used to import the geospatial data set: `hunan`, a polygon feature layer in ESRI shapefile format.

> ***Student Note***: The geospatial objects are polygon features. There are a total of **88 features and 8 fields** in `hunan` simple feature data frame. `hunan` is in **wgs84 coordinate system**.

```{r}
hunan = st_read(dsn = "data/geospatial",                   layer = "Hunan")
```

### 3.2 Importing csv file

The **read_csv()** (under readr package) is used to import the aspatial data set: `hunan_2012`, a csv file.

> ***Student Note***: The `hunan_2012` tibble data frame contains **88 rows and 29 columns**. There are two columns with character data - County and City.

```{r}
hunan2012 = read_csv("data/aspatial/Hunan_2012.csv")
```

### 3.3 Performing Relational Join

The attribute table of the spatial polygons data frame, `hunan`, is updated using the attribute fields of the tibble data frame, `hunan2012` using **left_join()** (under dplyr package). In order to retain the geospatial properties, the left data frame must the sf data.frame (i.e.Â `hunan`)

> ***Student Note***: Without explicitly stating the "by" argument for left_join(), the two tables are joined by the 'County' columns.

> ***Student Note***: The geometry variable is automatically selected (when using dplyr package - part of tidyverse).

```{r}
hunan_GDPPC = left_join(hunan,hunan2012) %>%
  select(1:4, 7, 15)

hunan_GDPPC
```

### 3.4 Visualising Regional Development Indicator

A basemap and a choropleth map are prepared usign **qtm()** (under tmap package) to visualise the 2012 Gross Domestic Product Per Capita (GDPPC).

```{r}
tmap_mode("plot")
tm_shape(hunan_GDPPC) +
  tm_fill("GDPPC", 
          style = "quantile", 
          palette = "Blues",
          title = "GDPPC") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Distribution of GDP per capita by district, Hunan Province",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)
```

## 4 Deriving Contiguity Spatial Weights

By and large, there are two type of spatial weights, they are contiguity weights and distance-based weights. In this section, the contiguity spatial weights are derived by using **sfdep** package.

The two steps for deriving contiguity spatial weights are:

1.  Identify contiguity neighbour list using **st_contiguity()** (under sfdep package); and

2.  Derive contiguity spatial weights using **st_weights()** (under **sfdep** package).

### 4.1 Identifying Contiguity Neighbours Using Queen's Method

The **st_contiguity()** (under sfdep package) is used to derive a contiguity neighbour list using Queen's method.

> ***Student Note***: The "queen" argument in the function is set to either TRUE (default) or FALSE. The TRUE option will return a list of first order neighbours using the Queen's contiguity criteria; the FALSE option will return a list of first order neighbours using the Rook's method. According to Queen's criteria, two regions are considered neighbours if they share any part of their boundary (even if it is a single point). This results in a more inclusive definition of neighbour relationships.

> ***Student Note***: The summary report below shows that there are **88 area units** in `nb_queen`. The **most connected area unit (85) has 11 neighbours**. The **least connected area units (30 and 65) have only one neighbour each**.

> **Student Note**: The .before = 1 argument indicates that the new column, "nb", should appear at the front.

```{r}
nb_queen = hunan_GDPPC %>% 
  mutate(nb = st_contiguity(geometry),
         .before = 1)

summary(nb_queen$nb)

nb_queen
```

One of the advantage of **sfdep** over **spdep** is that the former's output is an sf tibble data frame.

```{r}
kable(head(nb_queen,
           n = 10))
```

### 4.2 Identifying Contiguity Neighbours Using Rooks' Method

The **st_contiguity()** (under sfdep package) is used to derive a contiguity neighbour list using Rooks' method.

> ***Student Note***: The "queen" argument in the function is set to either TRUE (default) or FALSE. The TRUE option will return a list of first order neighbours using the Queen's contiguity criteria; the FALSE option will return a list of first order neighbours using the Rook's method. According to Rook's criteria, two regions are considered neighbours if they share an entire edge (but not corners). This results in a stricter definition neighbour relationships.

> ***Student Note***: The summary report below shows that there are **88 area units** in `nb_rook`. The **most connected area unit (85) has 10 (not 11) neighbours**. The **least connected area units (30 and 65) have only one neighbour each**.

> **Student Note**: The ".before = 1" argument in **mutate()** indicates that the new column, "nb" should appear at the front.

```{r}
nb_rook = hunan_GDPPC %>% 
  mutate(nb = st_contiguity(geometry,
                            queen = FALSE),
         .before = 1)

summary(nb_rook$nb)

nb_rook
```

### 4.3 Identifying Higher Order Neighbours

The **st_nb_lag_cumul()** (under sfdep package) is used to identify higher order contiguity neighbours. The result contains both first and second order neighbors because the order is set to 2.

> ***Student Note***: The summary report below shows that there are **88 area units** in `nb2_queen`. For order set at 2, the **most connected area unit (56) has 33 neighbours**. The **least connected area units (30 and 88) have five neighbours each**.

```{r}
nb2_queen =  hunan_GDPPC %>% 
  mutate(nb = st_contiguity(geometry),
         nb2 = st_nb_lag_cumul(nb, 2),
         .before = 1)

summary(nb2_queen$nb2)

nb2_queen
```

### 4.4 Deriving Contiguity Weights Using Queen's Method

The **st_weights()** (under **sfdep** package) is used to compute the contiguity weights.

The arguments for st_weights() include:

-   nb: A neighbour list object as created by st_neighbours().

-   style: Default "W" for row standardised weights. This value can also be "B", "C", "U", "minmax", and "S".

    -   The "**style**" argument set to "W" specifies a **binary spatial weight matrix**, where the presence of a spatial relationship is indicated by 1, and absence by 0. **All neighbouring units are considered equal in terms of their impact on the target unit, reflecting a uniform spatial relationship.**

    -   If the "**style**" argument is set to "B", it specifies a **binary spatial lag matrix**, where the presence of a spatial relationship is indicated by 1, and absence by 0. At the same time, the **direction of the connection is considered** for the calculations.

    -   "C" is globally standardised (sums over all links to n), "U" is equal to "C" divided by the number of neighbours (sums over all links to unity), while "S" is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. (1999) (sums over all links to n).

-   allow_zero: If TRUE, assigns zero as lagged value to zone without neighbors.

```{r}
wm_q = hunan_GDPPC %>%
  mutate(nb = st_contiguity(geometry),
         wt = st_weights(nb,
                         style = "W"),
         .before = 1)

wm_q
```

## 5 Distance-based Weights

There are three popularly used distance-based spatial weights, they are:

1.  Fixed distance weights;

2.  Adaptive distance weights; and

3.  Inverse distance weights (IDW).

### 5.1 Deriving Fixed Distance Weights

Firstly, the upper limit for the distance band would need to be derived.

-   **st_nb_dists()** (under sfdep package) is used to calculate the nearest neighbour distance. The output is a list of distances for each observation's neighbors list.

-   **unlist()** (under base package) is then used to return the output as a vector so that the summary statistics of the nearest neighbour distances can be derived.

The summary statistics report below shows that the maximum nearest neighbour distance is 65.80 km. Hence, the threshold value is set at 66 km to ensure that each area will have at least one neighbour.

```{r}
geo = st_geometry(hunan_GDPPC)
nb = st_knn(geo, longlat = TRUE)
dists = unlist(st_nb_dists(geo, nb))

summary(dists)
```

The fixed distance weights are then computed.

-   **st_dists_band()** (under sfdep package) is used to identify neighbors based on a distance band (i.e. 66km). The output is a list of neighbours (i.e. nb).

-   **st_weights()** is then used to calculate polygon spatial weights of the nb list. Note that:

    -   Default "style" argument is set to "W" for row standardized weights, and

    -   Default "allow_zero" is set to TRUE, which assigns zero as lagged value to zone without neighbors.

```{r}
wm_fd = hunan_GDPPC %>%
  mutate(nb = st_dist_band(geometry,
                           upper = 66),
               wt = st_weights(nb),
               .before = 1)

wm_fd
```

> ***Student Note***: The wt values of each row adds to 1 as a result of row standardisation. The wt values of each neighbour of a polygon are the same as a result of fixed distance weights.

### 5.2 Deriving Adaptive Distance Weights

One of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have less neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.

It is possible to control the numbers of neighbours directly using **st_knn()** (under sfdep package).

The adaptive distance weights are then computed.

-   **st_knn()** (under sfdep package) is used to identify neighbors based on k (i.e.Â k = 8 indicates the nearest eight neighbours). The output is a list of neighbours (i.e. nb).

-   **st_weights()** (under sfdep package) is then used to calculate polygon spatial weights of the nb list. Note that:

    -   Default "style" argument is set to "W" for row standardized weights, and

    -   Default "allow_zero" is set to TRUE, which assigns zero as lagged value to zone without neighbors.

```{r}
wm_ad = hunan_GDPPC %>% 
  mutate(nb = st_knn(geometry,
                     k=8),
         wt = st_weights(nb),
               .before = 1)

wm_ad
```

### 5.3 Deriving Inverse Distance Weights

IDW assigns higher weights to neighbours that are closer and lower weights to neighbours that are further away.

The inverse distance weights (IDW) are then computed.

-   **st_contiguity()** (under sfdep package) is used to identify the neighbours by using contiguity criteria. The output is a list of neighbours (i.e. nb).

-   **st_inverse_distance()** (under sfdep package) is then used to calculate inverse distance weights of neighbours on the nb list.

```{r}
wm_idw = hunan_GDPPC %>%
  mutate(nb = st_contiguity(geometry),
         wts = st_inverse_distance(nb, geometry,
                                   scale = 1,
                                   alpha = 1),
         .before = 1)

wm_idw
```

## 6 Global Measures of Spatial Association

### 6.1 Computing Global Moran's I

The **global_moran()** (under sfdep package) is used to compute the Moran's I value. The output is a tibble data frame (unlike when using spdep package).

> ***Student Note***: I value of 0.301 represents Moran's I value. A positive value in this case indicates positive spatial autocorrelation, meaning that similar values tend to be clustered, i.e., neighbouring regions are more alike than would be expected by random chance.

> ***Student Note***: K value of 7.64 represents the average number of neighbours.

```{r}
moranI = global_moran(wm_q$GDPPC,
                       wm_q$nb,
                       wm_q$wt)

glimpse(moranI)
```

### 6.2 Performing Global Moran's I Test

The **global_moran_test()** (under sfdep package) is used to perform the Moran's I test.

-   The default for "alternative" argument is "two.sided". Other supported arguments are "greater" or "less".

-   By default, the "randomization" argument is TRUE. If FALSE, the assumption of normality is set.

> ***Student Note***: **Interpretation**
>
> -   The Moran's I statistic of 0.300749970 suggests a **positive spatial autocorrelation in the GDPPC variable**.
>
> -   The **p-value of 1.095e-06 is very small, indicating a strong evidence to reject the null hypothesis of spatial randomness** and support the alternative hypothesis.
>
> -   **The alternative hypothesis is that there is a *significant* positive autocorrelation in the GDPPC**, i.e., neighbouring regions tend to have similar GDPPC values (cluster together).

```{r}
global_moran_test(wm_q$GDPPC,
                       wm_q$nb,
                       wm_q$wt)
```

### 6.3 Performing Global Moran's I Permutation Test

In practice, Monte Carlo simulation is used to perform the statistical test. The **global_moran_perm()** (under sfdep package) is used to do so.

-   The **set.seed()** is used to ensure reproducibility.

-   The "+ 1" is typically added to the **number of simulations** because the [observed value]{.underline} is also included in the distribution of [simulated values]{.underline}.

> ***Student Note***: **Interpretation**
>
> -   The Moran's I statistic of 0.30075 suggests a **positive spatial autocorrelation in the GDPPC variable**.
>
> -   The **p-value of \<2.2e-16 is less than the commonly used significance level of 0.05, indicating a strong evidence to reject the null hypothesis of spatial randomness** and support the alternative hypothesis.
>
> -   **The alternative hypothesis is that there is a *significant* positive autocorrelation in the GDPPC**, i.e., neighbouring regions tend to have similar GDPPC values (cluster together).
>
> -   **The rank of 1,00 of the observed Moran's I statistic compared to the simulated values means that the observed spatial pattern is unlikely to have occurred by random chance.**

```{r}
set.seed(1234)

global_moran_perm(wm_q$GDPPC,
                       wm_q$nb,
                       wm_q$wt,
                  nsim = 99)
```

## 7 Local Measures of Spatial Association

### 7.1 Computing Local Moran's I

The **local_moran()** (under sfdep package) is used to compute the local Moran's I value of GDPPC at county level. The output is a tibble data frame (unlike when using spdep package).

-   **unnest()** (under tidyr package) is used to expand a list-column containing data frames into rows and columns.

```{r}
lisa = wm_q %>%
  mutate(local_moran = local_moran(
    GDPPC, nb, wt, nsim = 99),
    .before = 1) %>%
  unnest(local_moran)
```

The output of local_moran() is a sf data frame containing the columns ii, eii, var_ii, z_ii, p_ii, p_ii_sim, and p_folded_sim.

-   *ii*: Local moran statistic.

-   *eii*: Expectation of local moran statistic; for local_moran_perm(), the permutation sample means.

-   *var_ii*: Variance of local moran statistic; for local_moran_perm(), the permutation sample standard deviations.

-   *z_ii*: Standard deviation of local moran statistic; for local_moran_perm(), based on permutation of sample means and standard deviations

-   *p_ii*: p-value of local moran statistic using pnorm(); for local_moran_perm(), using standard deviation based on permutation of sample means and standard deviations

-   *p_ii_sim*: For local_moran_perm(), rank() and punif() of observed statistic rank for \[0, 1\] p-values using "alternative="

-   *p_folded_sim*: The simulation folded \[0, 0.5\] range ranked p-value.

-   *skewness*: For local_moran_perm(), the output of e1071::skewness() for the permutation samples underlying the standard deviates

-   *kurtosis*: For local_moran_perm(), the output of e1071::kurtosis() for the permutation samples underlying the standard deviates.

> ***Student Note***:
>
> -   **Positive local Moran's I values indicate a location surrounded by similar values** (High-High or Low-Low).
>
> -   **Negative values indicate a location surrounded by dissimilar values** (High-Low or Low-High).
>
> -   **Values near zero suggest no significant local spatial autocorrelation.**

### 7.2 Visualising Local Moran's I

The **tmap** functions are used to plot a choropleth map using the values in the "ii" field of `lisa`.

```{r}
tmap_mode("plot")
tm_shape(lisa) +
  tm_fill("ii") + 
  tm_borders(alpha = 0.5) +
  tm_view(set.zoom.limits = c(6,8)) +
  tm_layout(main.title = "local Moran's I of GDPPC",
            main.title.size = 0.8)
```

The choropleth shows evidence of both positive and negative ii values. However, it is useful to consider the p-values for each of these values to see their statistical significance. A choropleth map of Moran's I p-values ("p_ii_sim") using functions of **tmap** package is plotted below.

```{r}
tmap_mode("plot")
tm_shape(lisa) +
  tm_fill("p_ii_sim",
          breaks = c(0, 0.001, 0.01, 0.05, 1),
              labels = c("0.001", "0.01", "0.05", "Not sig")) + 
  tm_borders(alpha = 0.5) +
   tm_layout(main.title = "p-value of local Moran's I",
            main.title.size = 0.8)
```

### 7.3 Visualising Local Moran's I and p-value

The two maps are then plotted next to each other.

> ***Student Note***: **Interpretation**
>
> -   Most of the p-values show that the Moran's I values are not statistically significant.
>
> -   Interpreting the Moran's I values together with the p-values, there is **statistically significant positive autocorrelation for the GDPPC of counties in the northeast region of Hunan**.

```{r}
tmap_mode("plot")
map1 = tm_shape(lisa) +
  tm_fill("ii") + 
  tm_borders(alpha = 0.5) +
  tm_view(set.zoom.limits = c(6,8)) +
  tm_layout(main.title = "local Moran's I of GDPPC",
            main.title.size = 0.8)

map2 = tm_shape(lisa) +
  tm_fill("p_ii",
          breaks = c(0, 0.001, 0.01, 0.05, 1),
              labels = c("0.001", "0.01", "0.05", "Not sig")) + 
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "p-value of local Moran's I",
            main.title.size = 0.8)

tmap_arrange(map1, map2, ncol = 2)
```

### 7.4 Visualising Local Indicators of Spatial Association (LISA) Map

The LISA map is a categorical map showing outliers and clusters. There are two types of outliers namely: High-Low and Low-High outliers. Likewise, there are two type of clusters namely: High-High and Low-Low clusters. **In fact, LISA map is an interpreted map by combining local Moran's I of geographical areas and their respective p-values.**

In the `lisa` sf data frame, we can find three fields contain the LISA categories. They are *mean*, *median* and *pysal*. In general, classification in *mean* will be used.

> ***Student Note***:
>
> -   The plot confirms the earlier interpretation above that there is a statistically significant positive autocorrelation for the GDPPC of counties in the northeast region of Hunan (i.e., High-High clusters).
>
> -   The plot also highlighted other significant clusters: Low-High for two counties in northeast region, and Low-Low for a county in central-west region.

```{r}
lisa_sig = lisa  %>%
  filter(p_ii < 0.05)

tmap_mode("plot")
tm_shape(lisa) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(lisa_sig) +
  tm_fill("mean") + 
  tm_borders(alpha = 0.4)
```

## 8 Hot Spot and Cold Spot Area Analysis

Hot Spot and Cold Spot Area Analysis (HCSA) uses spatial weights to identify locations of statistically significant hot spots and cold spots in an spatially weighted attribute that are in proximity to one another based on a calculated distance. The analysis groups features when similar high (hot) or low (cold) values are found in a cluster. The polygon features usually represent administration boundaries or a custom grid structure.

### 8.1 Computing Local Gi\* Statistics

The inverse distance weights (IDW) are already computed in section 5.3 above.

```{r}
wm_idw
```

The **local_gstar_perm()** (under sfdep package) is used to compute local Gi\* statistics.

```{r}
HCSA = wm_idw %>% 
  mutate(local_Gi = local_gstar_perm(
    GDPPC, nb, wt, nsim = 99),
         .before = 1) %>%
  unnest(local_Gi)

HCSA
```

### 8.2 Visualising Gi\*

The Gi\* values are plotted using functions in the **tmap** package.

> ***Student Note***: Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.

> ***Student Note***: **Interpretation**
>
> -   The positive Gi\* values for the northeast region indicate High-High clustering, while the negative Gi\* values for the northwest and central-west regions indicate Low-Low clustering.

```{r}
tmap_mode("plot")
tm_shape(HCSA) +
  tm_fill("gi_star") + 
  tm_borders(alpha = 0.5) +
  tm_view(set.zoom.limits = c(6,8))
```

### 8.3 Visualising p-value of Hot Spot and Cold Spot Area Analysis

The p-values of the HCSA are plotted using functions in the **tmap** package.

```{r}
tmap_mode("plot")
tm_shape(HCSA) +
  tm_fill("p_sim",
          breaks = c(0, 0.001, 0.01, 0.05, 1),
              labels = c("0.001", "0.01", "0.05", "Not sig")) + 
  tm_borders(alpha = 0.5)
```

### 8.4 Visualising Local Hot Spot and Cold Spot Area Analysis

The two maps are then plotted next to each other.

> ***Student Note***: **Interpretation**
>
> -   There are no statistically significant Hot Spots or Cold Spots in the western region.
>
> -   For the northeast region, there are statistically High-High clusters.

```{r}
tmap_mode("plot")
map1 = tm_shape(HCSA) +
  tm_fill("gi_star") + 
  tm_borders(alpha = 0.5) +
  tm_view(set.zoom.limits = c(6,8)) +
  tm_layout(main.title = "Gi* of GDPPC",
            main.title.size = 0.8)

map2 = tm_shape(HCSA) +
  tm_fill("p_sim",
          breaks = c(0, 0.001, 0.01, 0.05, 1),
          labels = c("0.001", "0.01", "0.05", "Not sig")) + 
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "p-value of Gi*",
            main.title.size = 0.8)

tmap_arrange(map1, map2, ncol = 2)
```

### 8.5 Visualising Hot Spot and Cold Spot Areas

The significant (i.e., p-values less than 0.05) hot spot and cold spot areas are then plotted by using **tmap** functions.

The plot below reveals one hot spot area and one cold spot area. The hot spot area coincides with the High-High cluster identified using local Moran's I method. Both local Moran's I and Gi\* values are derived from the spatially weighted values, and both are measures to assess spatial autocorrelation. LISA identifies local spatial patterns while Gi\* is useful for identifying hot spots and cold spot areas.

```{r}
HCSA_sig = HCSA  %>%
  filter(p_sim < 0.05)

tmap_mode("plot")
tm_shape(HCSA) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(HCSA_sig) +
  tm_fill("gi_star") + 
  tm_borders(alpha = 0.4)
```

## 9 Emerging Hot Spot Analysis

Emerging Hot Spot Analysis (EHSA) is a spatio-temporal analysis method for revealing and describing how hot spot and cold spot areas evolve over time.

The analysis consist of four main steps:

1.  Build a space-time cube;

2.  Calculate Getis-Ord local Gi\* statistic for each bin by using an False Discovery rate (FDR) correction;

    > ***Student Note***: FDR is used to adjust p-values for multiple comparisons to address the issue of inflated Type I errors when conducting many test simultaneously, such as when assessing the significance of local spatial clusters.

3.  Evaluate the hot and cold spot trends by using Mann-Kendall trend test;

4.  Categorise each study area location by referring to the resultant trend z-score and p-value for each location within data, and with the hot spot z-score and p-value for each bin.

### 9.1 Importing Attribute Table

The **read_csv()** (under readr package) is used to import Hunan_GDPPC.csv.

```{r}
GDPPC = read_csv("data/aspatial/Hunan_GDPPC.csv")
```

> ***Student Note***: 'Year' variable is numerical data type (not datetime).

### 9.2 Creating a Time Series Cube

The **spacetime()** (under sfdep package) is used to create a spacetime cube.

```{r}
GDPPC_st = spacetime(GDPPC, hunan,
                     .loc_col = "County",
                     .time_col = "Year")
```

The **is_spacetime_cube()** (under sfdep package) is used to verify if GDPPC_st is indeed a spacetime cube object. The TRUE return below confirms that GDPPC_st object is indeed a spacetime cube.

```{r}
is_spacetime_cube(GDPPC_st)
```

### 9.3 Computing Gi\* Statistics

The Gi\* statistics is then computed by identifying neighbours, deriving an inverse distance weights, and then calculating the local Gi\* for each location.

-   **activate()** (under dplyr package) is used to activate the geometry context

-   **mutate()** (under dplyr package) is used to create two new columns "nb" and "wt".

-   **set_nbs()** and **set_wts()** (both under sfdep package) are used to activate the data context again and copy over the nb and wt columns to each time-slice.

    -   The row order is very important so do not rearrange the observations after using set_nbs() or set_wts().

```{r}
GDPPC_nb = GDPPC_st %>%
  activate("geometry") %>%
  mutate(nb = include_self(st_contiguity(geometry)),
         wt = st_inverse_distance(nb, geometry,
                                  scale = 1,
                                  alpha = 1),
         .before = 1) %>%
  set_nbs("nb") %>%
  set_wts("wt")

head(GDPPC_nb)
```

The new columns are then used to calculate the local Gi\* for each location. This is done by grouping by "Year" and using **local_gstar_perm()** (under sfdep package). Then, unnest() (under tidyr package) is used to unnest the "gi_star" column of the newly created gi_stars data frame.

```{r}
gi_stars = GDPPC_nb %>%
  group_by(Year) %>%
  mutate(gi_star = local_gstar_perm(
    GDPPC, nb, wt)) %>%
  unnest(gi_star)

head(gi_stars)
```

### 9.4 Mann-Kendall Test

Each location is then evaluated for a trend using the Mann-Kendall test. For illustration, Changsha (provincial capital of Hunan) is tested below.

> ***Student Note***: The \|\> symbol is called the "pipe-forward" operator, and it is used in the context of the magrittr package in R. It was introduced in magrittr to provide an alternative to the %\>% pipe operator. The \|\> operator is part of the native syntax of R since version 4.1.0. Just like %\>%, the \|\> operator is used to chain operations together, passing the result of one expression as the first argument to the next.

```{r}
cbg = gi_stars %>% 
  ungroup() %>% 
  filter(County == "Changsha") |> 
  select(County, Year, gi_star)

head(cbg)
```

The result is then plotted using functions in **ggplot2** package.

```{r}
ggplot(data = cbg, 
       aes(x = Year, 
           y = gi_star)) +
  geom_line() +
  theme_light()
```

An interactive plot can also be created using **ggplotly()** (under plotly package).

```{r}
p = ggplot(data = cbg, 
       aes(x = Year, 
           y = gi_star)) +
  geom_line() +
  theme_light()

ggplotly(p)
```

The p-value for the test is then calculated. In the result below, sl is the p-value. This result tells us that there is a slight upward but insignificant trend.

```{r}
cbg %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>%
  unnest_wider(mk)
```

The above test can be replicated for each location using group_by() (under dplyr package).

```{r}
ehsa = gi_stars %>%
  group_by(County) %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>%
  unnest_wider(mk)

head(ehsa)
```

The values are then arranged in order of the "tau" values to show significant emerging hot or cold spots.

```{r}
emerging = ehsa %>% 
  arrange(sl, abs(tau)) %>% 
  slice(1:5)

head(emerging)
```

### 9.5 Performing Emerging Hot Spot Analysis

The EHSA analysis is then performed by using **emerging_hotspot_analysis()** (under sfdep package).

-   The main argument is a spacetime object x (i.e. GDPPC_st).

-   The ".var" argument is the quoted name of the variable of interest (i.e. GDPPC).

-   The "k" argument is used to specify the number of time lags. The default = 1 means comparing time series sequentially.

-   The "nsim" argument states the number of simulations to be performed.

```{r}
ehsa = emerging_hotspot_analysis(
  x = GDPPC_st, 
  .var = "GDPPC", 
  k = 1, 
  nsim = 99
)

head(ehsa)
```

### 9.6 Visualising Distribution of Emerging Hot Spot Analysis Classes

The functions in ggplot2 package are used to reveal the distribution of EHSA classes using bar chart.

The bar chat below shows that sporadic cold spots class has the high numbers of counties.

```{r}
ggplot(data = ehsa,
       aes(x = classification)) +
  geom_bar()
```

### 9.7 Visualising Emerging Hot Spot Analysis

The `hunan` and `ehsa` data frames are joined.

```{r}
hunan_ehsa = hunan %>%
  left_join(ehsa,
            by = join_by(County == location))
```

The geographic distribution of EHSA classes are then plotted using functions in **tmap** package.

```{r}
ehsa_sig = hunan_ehsa  %>%
  filter(p_value < 0.05)
tmap_mode("plot")

tm_shape(hunan_ehsa) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(ehsa_sig) +
  tm_fill("classification") + 
  tm_borders(alpha = 0.4)
```

> ***Student Note*****: Interpretation**
>
> -   The grey areas mean that there are no statistically significant values. This is not the same as no pattern (which could be statistically significant).
>
> -   There is a mix of different types of hot or cold spot patterns in Hunan.
>
> -   Focusing on the class with the most number of counties, the presence of sporadic cold spots indicates that there are many counties that are on-again, off-again cold spots over time (i.e., spatial concentration of low GDPPC values).
>
> -   There are also a number of sporadic hot spots, which indicates that there are some counties that are on-again, off-again hot spots over time (i.e., spatial concentration of high GDPPC values).
>
> -   Overall, the high number of sporadic classes (both hot and cold) indicates that the trends of spatial concentration of GDPPC values in many counties in Hunan are not stable or linear (increasing or decreasing over time) but are more unpredictable. This also means that spatial patterns in GDPPC are not static across different time periods.

[**\~\~\~ End of In-class Exercise 2 \~\~\~**]{.smallcaps}
